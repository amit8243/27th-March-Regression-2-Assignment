{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24dd7c-4a70-4d12-94eb-49f5d71e49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression-2 Assignment\n",
    "\"\"\"Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\"\"\"\n",
    "Ans:R-squared is a statistical measure of how close the data are to the fitted regression line. It is also\n",
    "known as the coefficient of determination, or the coefficient of multiple determination for multiple \n",
    "regression. It is calculated by taking the sum of squares of the differences between the observed values\n",
    "and the predicted values, and dividing it by the sum of squares of the differences between the observed \n",
    "values and the mean of observed values. R-squared is a number between 0 and 1, where 0 indicates that the\n",
    "model explains none of the variability of the response data around its mean, and 1 indicates that the \n",
    "model explains all the variability of the response data around its mean. In general, the higher the \n",
    "R-squared, the better the model fits your data.\n",
    "\n",
    "\"\"\"Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\"\"\"\n",
    "Ans: \n",
    "Adjusted R-squared is a measure of how well a regression model fits the data. It is similar to R-squared,\n",
    "but it takes into account the number of predictors in the model. The adjusted R-squared increases only \n",
    "if the new term improves the model more than would be expected by chance. It decreases when a predictor \n",
    "improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s \n",
    "usually not. The regular R-squared only increases when a new term improves the model, regardless of how \n",
    "many predictors are in the model.\n",
    "\n",
    "In summary, adjusted R-squared takes into account the number of predictors in the model, while regular\n",
    "R-squared does not.\n",
    "\n",
    "\"\"\"Q3. When is it more appropriate to use adjusted R-squared?\"\"\"\n",
    "Ans: Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It \n",
    "adjusts for the number of predictors in the model, so it can be used to compare models with different \n",
    "numbers of predictors and determine which model is more accurate.\n",
    "\n",
    "It is also more appropriate when the number of observations is small relative to the number of predictors. \n",
    "In this case, adjusted R-squared can be used to determine which model is more accurate despite the small \n",
    "sample size.\n",
    "\n",
    "\"\"\"Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\"\"\"\n",
    "Ans: \n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are all metrics \n",
    "used to evaluate the performance of a regression model. \n",
    "\n",
    "RMSE is calculated by taking the square root of the mean of the squared differences between the predicted \n",
    "and actual values. It is a measure of the average magnitude of the errors in a set of predictions, without \n",
    "considering their direction.\n",
    "\n",
    "MSE is calculated by taking the mean of the squared differences between the predicted and actual values. \n",
    "It is a measure of the average magnitude of the errors in a set of predictions, without considering their \n",
    "direction.\n",
    "\n",
    "MAE is calculated by taking the mean of the absolute differences between the predicted and actual values. \n",
    "It is a measure of the average magnitude of the errors in a set of predictions, without considering their \n",
    "direction.\n",
    "\n",
    "These metrics represent the average magnitude of the errors in a set of predictions, without considering \n",
    "their direction. They are useful for comparing different models and determining which one is more accurate.\n",
    "\n",
    "\"\"\"Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\"\"\"\n",
    "Ans: Advantages of RMSE, MSE, and MAE:\n",
    "\n",
    "1. RMSE (Root Mean Squared Error): It is the most popular evaluation metric for regression problems as it\n",
    "penalizes large errors more than small errors. This helps to identify outliers in the data set.\n",
    "\n",
    "2. MSE (Mean Squared Error): It is a good measure of how close the predicted values are to the actual \n",
    "values. It is also easy to interpret and calculate.\n",
    "\n",
    "3. MAE (Mean Absolute Error): It is a robust metric that is not affected by outliers in the data set. It \n",
    "is also easy to interpret and calculate.\n",
    "\n",
    "Disadvantages of RMSE, MSE, and MAE:\n",
    "\n",
    "1. RMSE (Root Mean Squared Error): It is sensitive to outliers in the data set which can lead to \n",
    "inaccurate results.\n",
    "\n",
    "2. MSE (Mean Squared Error): It is not a robust metric and can be affected by outliers in the data set.\n",
    "\n",
    "3. MAE (Mean Absolute Error): It does not take into account the magnitude of the errors which can lead to \n",
    "inaccurate results.\n",
    "\n",
    "Overall, RMSE, MSE, and MAE are all useful evaluation metrics for regression analysis. However, it is \n",
    "important to consider the advantages and disadvantages of each metric before deciding which one to use.\n",
    "\n",
    "\"\"\"Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\"\"\"\n",
    "\n",
    "Lasso regularization is a type of regularization technique used in linear regression models. It adds a \n",
    "penalty term to the cost function, which is the sum of the squared errors between the predicted values \n",
    "and the actual values. The penalty term is proportional to the absolute value of the coefficients of the \n",
    "model. This encourages the coefficients to be close to zero, which in turn reduces the complexity of the \n",
    "model and helps to avoid overfitting.\n",
    "\n",
    "Lasso regularization differs from Ridge regularization in that it uses an absolute value penalty instead \n",
    "of a squared penalty. This means that it is more likely to set some of the coefficients to zero, which \n",
    "can be useful for feature selection.\n",
    "\n",
    "Lasso regularization is more appropriate when there are a large number of features and some of them are \n",
    "not important for the model. It is also useful when there is multicollinearity in the data, as it can \n",
    "help to reduce the complexity of the model.\n",
    "\n",
    "In contrast, Ridge regularization is more appropriate when there are a large number of features and all \n",
    "of them are important for the model. It is also useful when there is multicollinearity in the data, as \n",
    "it can help to reduce the variance of the model.\n",
    "\n",
    "\"\"\"Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\"\"\"\n",
    "Ans: Regularized linear models help to prevent overfitting by adding a penalty term to the loss function.\n",
    "This penalty term is usually the sum of the weights of the model, multiplied by a regularization \n",
    "parameter. The regularization parameter controls how much the model is penalized for having large weights. \n",
    "By increasing the regularization parameter, the model is encouraged to have smaller weights, which reduces\n",
    "the complexity of the model and helps to prevent overfitting.\n",
    "\n",
    "For example, in ridge regression, the loss function is modified to include a penalty term for large \n",
    "weights. The modified loss function looks like this:\n",
    "\n",
    "Loss = (Predicted - Actual)^2 + λ * (Weight^2)\n",
    "\n",
    "Where λ is the regularization parameter. By increasing the value of λ, the model is encouraged to have \n",
    "smaller weights, which helps to prevent overfitting.\n",
    "\n",
    "\"\"\"Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\"\"\"\n",
    "Ans: Regularized linear models have several limitations. First, they are limited to linear relationships \n",
    "between the independent and dependent variables. This means that they cannot capture non-linear \n",
    "relationships, which may be important in some cases. Second, regularized linear models are sensitive to \n",
    "outliers and can be easily overfit. Third, regularized linear models are not suitable for dealing with \n",
    "high-dimensional data, as the number of parameters increases exponentially with the number of features.\n",
    "Finally, regularized linear models may not be the best choice for regression analysis when there is a \n",
    "large amount of noise in the data or when the data is highly non-linear.\n",
    "\n",
    "\"\"\"Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\"\"\"\n",
    "Ans:Model B would be the better performer, as it has a lower MAE. However, there are limitations to this \n",
    "choice of metric. The RMSE is more sensitive to outliers than the MAE, so if there are any outliers in the\n",
    "data set, Model A may be a better choice. Additionally, the RMSE is more useful for comparing models with \n",
    "different scales, while the MAE is more useful for comparing models with similar scales.\n",
    "\n",
    "\"\"\"Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\"\"\"\n",
    "Ans: It is difficult to determine which model would be the better performer without seeing the data and \n",
    "evaluating the models. Generally, Lasso regularization is better for feature selection, while Ridge \n",
    "regularization is better for reducing model complexity and multicollinearity. Therefore, if feature \n",
    "selection is important, Model B (Lasso regularization) would be the better choice. However, if reducing \n",
    "model complexity is more important, Model A (Ridge regularization) would be the better choice.\n",
    "\n",
    "There are trade-offs and limitations to both types of regularization. Lasso regularization can lead to \n",
    "underfitting, while Ridge regularization can lead to overfitting. Additionally, the choice of \n",
    "regularization parameter can have a significant impact on the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
